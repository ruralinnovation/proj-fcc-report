[
  {
    "objectID": "zero_dl_up.html",
    "href": "zero_dl_up.html",
    "title": "EDA on 0/0 BSL",
    "section": "",
    "text": "TODO: update the SQL query\nCode\nsource(\"R/table_with_options.R\")\n# very lazish function, col should be a string \nagg_count &lt;- function(dat, col) {\n    agg &lt;- aggregate(cbind(count = dat$count),\n                     list(name_col = dat[[col]]),\n                      sum)\n    colnames(agg) &lt;- c(col, \"count\")\n    return(agg)\n}\nThe goals of this page is storing a quick EDA about broadband services locations with 0 MBps uploads and 0 MBps downloads. To be concise we are going to call them 0/0 speeds.\nWe have counted every services that have been declared with 0/0 speeds and associated with their States, ISP and technology. To clarify that does not mean a location have 0/0 speeds only but that one “ISP x technology” is provided with this kind of service in this location.\nThe data used to provide most of the analysis was done with this 2 SQL queries. They were saved and stored in data/\nCode\nSELECT \n    state_abbr,\n    brand_name,\n    technology,\n    count(brand_name)\nFROM\n    staging.june23\nWHERE\n(max_advertised_download_speed = 0 AND\n    max_advertised_upload_speed = 0) = true\nGROUP BY brand_name, state_abbr, technology;\n\n-- first get all 0/0 then get all the non 0/0\n\nSELECT \n    state_abbr,\n    brand_name,\n    technology,\n    count(brand_name)\nFROM \n    staging.june23\nWHERE\n(max_advertised_download_speed = 0 AND\n    max_advertised_upload_speed = 0) = false\nGROUP BY brand_name, state_abbr, technology;\nCode\nzero_loc &lt;- read.csv(\"data/zero_dl_up.csv\")\nnot_zero &lt;- read.csv(\"data/not_zero_dl.csv\")",
    "crumbs": [
      "Home",
      "EDA",
      "EDA on 0/0 BSL"
    ]
  },
  {
    "objectID": "zero_dl_up.html#summary-by-technologies",
    "href": "zero_dl_up.html#summary-by-technologies",
    "title": "EDA on 0/0 BSL",
    "section": "Summary by technologies:",
    "text": "Summary by technologies:\n\n\nCode\nagg &lt;- agg_count(zero_loc, \"technology\") \nagg_not &lt;- agg_count(not_zero, \"technology\")\n\ntechnology &lt;- merge(agg, agg_not, by.x = \"technology\", \n                    by.y = \"technology\", all.x = TRUE, all.y = TRUE) \ncolnames(technology) &lt;- c(\"technology\",  \"cnt_zero_dl\", \"cnt_non_zero\")\ntechnology$rate_zero &lt;- round(technology$cnt_zero_dl / \n                (technology$cnt_zero_dl +  technology$cnt_non_zero), 4)\n\ntable_with_options(technology)\n\n\n\n\n\n\n\nWe do not mind too much 70 (Unlicensed Terrestrial Fixed Wireless) because we are filtering it out but we are keeping 71 (Licensed Terrestrial Fixed Wireless) , 72 (Licensed-by-Rule Terrestrial Fixed Wireless)and 10 (Copper Wire).\nTo take that into account I will filter out Unlicensed Terrestrial Fixed Wireless for the rest of this document. I also filtered out 60 and 61 to be consistant with our pipelines.",
    "crumbs": [
      "Home",
      "EDA",
      "EDA on 0/0 BSL"
    ]
  },
  {
    "objectID": "zero_dl_up.html#summary-by-isp",
    "href": "zero_dl_up.html#summary-by-isp",
    "title": "EDA on 0/0 BSL",
    "section": "Summary by ISP",
    "text": "Summary by ISP\n\n\nCode\nfilter_sat &lt;- c(60, 61, 70)\nzero_loc &lt;- zero_loc[which(! zero_loc$technology %in% filter_sat), ]\nnot_zero &lt;- not_zero[which(! not_zero$technology %in% filter_sat), ]\n\nagg &lt;- agg_count(zero_loc, \"brand_name\")\nagg_not &lt;- agg_count(not_zero, \"brand_name\")\n\nrate_zero &lt;- merge(agg, agg_not, \n                   by.x = \"brand_name\", by.y = \"brand_name\"\n                   , all.x = TRUE) \n\ncolnames(rate_zero) &lt;- c(\"brand_name\", \"cnt_zero_dl\", \"cnt_non_zero\")\nrate_zero$rate_zero &lt;- round(rate_zero$cnt_zero_dl /\n                 (rate_zero$cnt_zero_dl +  rate_zero$cnt_non_zero),\n                            4)\n\ntable_with_options(rate_zero[\n                    order(rate_zero$cnt_zero_dl, decreasing = TRUE),])\n\n\n\n\n\n\n\n\n402 ISPs are declaring services with 0/0 MBips (We have 2902 ISPs registered in FCC NBM)",
    "crumbs": [
      "Home",
      "EDA",
      "EDA on 0/0 BSL"
    ]
  },
  {
    "objectID": "zero_dl_up.html#sumamry-by-states",
    "href": "zero_dl_up.html#sumamry-by-states",
    "title": "EDA on 0/0 BSL",
    "section": "Sumamry by States",
    "text": "Sumamry by States\n\n\nCode\nst_agg_zero &lt;- agg_count(zero_loc, \"state_abbr\")\nst_agg_not &lt;- agg_count(not_zero, \"state_abbr\")\n\nst_agg &lt;- merge(st_agg_zero, st_agg_not, \n                by.x = \"state_abbr\", by.y = \"state_abbr\",\n                all.x = TRUE, all.y = TRUE)\ncolnames(st_agg) &lt;- c(\"ST\", \"cnt_zero_dl\", \"cnt_non_zero\")\nst_agg$rate_zero &lt;- round(st_agg$cnt_zero_dl /\n                    (st_agg$cnt_zero_dl +  st_agg$cnt_non_zero),\n                            4) \nlibrary(ggplot2)\n\nggplot(st_agg[!is.na(st_agg$rate_zero),], aes(rate_zero)) +\n  geom_boxplot(orientation = \"y\",\n  fill='#A4A4A4', color=\"black\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ntable_with_options(st_agg[order(st_agg$rate_zero, decreasing = TRUE), ])\n\n\n\n\n\n\n One point of concern is that services with 0/0 speeds could be generated for various reasons. One could be that some technology offer very low downloads/uploads and that is rounding to 0 an other could be that the location is not actually deserved but the ISP think it can do it.",
    "crumbs": [
      "Home",
      "EDA",
      "EDA on 0/0 BSL"
    ]
  },
  {
    "objectID": "rdof.html",
    "href": "rdof.html",
    "title": "RDOF EDA",
    "section": "",
    "text": "We had some previous works on RDOF for BCAT. It used shapefiles and gpkg to produce the output. This was not needed anymore as we could relly on census boundary (with less topological error) and just use the spreadsheet profided (.xls).\nIf we were about to change our workflow it was a good opportunity to test the end results.\nThe R script used for that can be find in the doc section of data-bead-etl repo.\nHere I will summarize some of the key results and dive a bit more on the trouble we ran into.",
    "crumbs": [
      "Home",
      "EDA",
      "RDOF EDA"
    ]
  },
  {
    "objectID": "rdof.html#key-results",
    "href": "rdof.html#key-results",
    "title": "RDOF EDA",
    "section": "Key results:",
    "text": "Key results:\n\nWe do not have any difference between our previous pipeline and the new one for “Authorized RDOF”\nIn 2024 they are no difference in “ready to authorized” to “authorized”: everything was authorized.\nWe also verified that RDOF data is matching to a Census block 2010 geoid.\nWe have data issue between “default”: ie we have two contradictory data sources.",
    "crumbs": [
      "Home",
      "EDA",
      "RDOF EDA"
    ]
  },
  {
    "objectID": "rdof.html#two-data-sources",
    "href": "rdof.html#two-data-sources",
    "title": "RDOF EDA",
    "section": "Two data sources:",
    "text": "Two data sources:\nOur previous works was on a release from the end of 2022 (16dec2022 according to the file name) and the new one end of 2023 (2023-12-20 according to the file name).\nThe version of 2022 have 266 994 and 286 892 for 2023. Our first thought was that it makes sense to have nore “default” as time go but some cases of default in 2022 are missing in 2023 (is it possible for a default RDOF to be not defaulted?).\n48 121 rows are only present in 2022 (they are the potentially weirds one). 68 019 rows are only present in 2023 version. Finally, 218873 are identics.\n\nOther small “hiccups”\nWe have some cases that were defaulted but still are present in authorized.: 47 cases, all in Wisconsin (County: 55043) and same company.\nThey will be removed from Authorized.\n\n\nUsing High Cost: Connect America Fund Broadband Map (CAF Map)\nThis data has a column for “Fund type” and one of this type is RDOF. It has 447 939 rows matching for RDOF. Unlike RDOF this data set is at the “location” scale, ie we can have multiple location per block.\nWe only get 54 825 Census Blocks, RDOF authorized has 489 811.\nThe vast majority of those are already present in RDOF data set, only 175 are ony present in the CAF data set.\nAccording to the metadata it seems that the CAF data set was produced in september 2023 but they did not provide their sources. It is hard to tract what was used.\nOur recomendation: use FCC source has much has possible and be mindfull when using other data sets.",
    "crumbs": [
      "Home",
      "EDA",
      "RDOF EDA"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sharing quick EDA about FCC data.\nTheir link: https://www.fcc.gov/BroadbandData\nFCC BDC: June 2023 release, downloaded 21-11-2023"
  },
  {
    "objectID": "hubb.html",
    "href": "hubb.html",
    "title": "High Cost: Connect America Fund Broadband Map (CAF Map)",
    "section": "",
    "text": "This dataset serves as the foundation for the Connect America Fund Broadband Map (CAF Map), which displays the geographic areas eligible for CAF support, as well as the specific fixed locations where carriers participating in the program have built out broadband service. The information in the CAF Map comes directly from carriers, which submit broadband deployment data annually through USAC’s High Cost Universal Broadband (HUBB) portal. The current CAF Map is based on data certified in the HUBB as of September 30, 2023. USAC independently verifies deployment to a random sample of reported locations each year to monitor carrier compliance with CAF build-out obligations, but not all data in the map has been subject to this review.\nHUBB\nSource Data\nAbout Page\n\n\n\nRecords represent a single address with awards deployed or will deploy in this year. The locations have a deployed date spanning from 2000-01-01 to 2023-09-19.\nA single address may have multiple households. This is represented by the column “Locations Deployed”.\nAvailable Fund Types are listed below but for the purpose of this request we filtered to ACAM I and ACAM II fund type.\nFund Type\n\nACAM\nACAM II\nAK Plan\nCAF II\nCAF II Auc\nCAF-BLS\nPR Fixed\nRBE\nRDOF\n\nTo determine the census block we used the field, Census Block, 2010 Census block of the deployment location.\nWe created 3 different flags:\n\nacam_i_flag: 1 indicates the block received ACAM I and 0 indicates it did not.\nacam_ii_flag: 1 indicates the block received ACAM II and 0 indicates it did not.\nhas_previous_funding: 1 indicates the block received ACAM I and/or ACAM II and 0 indicates it did not.\n\nWe produced two tables, proj_bead.acam_bl_tidy and proj_bead.acam_bl_wide.\nWhen generating the proj_bead.acam_bl_tidy table we had to crosswalk the 2010 block fips to the 2020 block fips using the proj_bead.cross_tab1020. We did this to create consistency with the other BEAD datasets.\nSome 2010 blocks with different funding flags were merged into one block in 2020. The 2020 block will be flagged as previously funded`if any of the 2010 blocks were previously funded.\nWide format table, proj_bead.acam_bl_wideis grouped by Census Block and summarized. The table is kept at 2010 blocks to avoid duplicate rows.",
    "crumbs": [
      "Home",
      "EDA",
      "High Cost: Connect America Fund Broadband Map (CAF Map)"
    ]
  },
  {
    "objectID": "hubb.html#acam-i-and-acam-ii",
    "href": "hubb.html#acam-i-and-acam-ii",
    "title": "High Cost: Connect America Fund Broadband Map (CAF Map)",
    "section": "",
    "text": "This dataset serves as the foundation for the Connect America Fund Broadband Map (CAF Map), which displays the geographic areas eligible for CAF support, as well as the specific fixed locations where carriers participating in the program have built out broadband service. The information in the CAF Map comes directly from carriers, which submit broadband deployment data annually through USAC’s High Cost Universal Broadband (HUBB) portal. The current CAF Map is based on data certified in the HUBB as of September 30, 2023. USAC independently verifies deployment to a random sample of reported locations each year to monitor carrier compliance with CAF build-out obligations, but not all data in the map has been subject to this review.\nHUBB\nSource Data\nAbout Page\n\n\n\nRecords represent a single address with awards deployed or will deploy in this year. The locations have a deployed date spanning from 2000-01-01 to 2023-09-19.\nA single address may have multiple households. This is represented by the column “Locations Deployed”.\nAvailable Fund Types are listed below but for the purpose of this request we filtered to ACAM I and ACAM II fund type.\nFund Type\n\nACAM\nACAM II\nAK Plan\nCAF II\nCAF II Auc\nCAF-BLS\nPR Fixed\nRBE\nRDOF\n\nTo determine the census block we used the field, Census Block, 2010 Census block of the deployment location.\nWe created 3 different flags:\n\nacam_i_flag: 1 indicates the block received ACAM I and 0 indicates it did not.\nacam_ii_flag: 1 indicates the block received ACAM II and 0 indicates it did not.\nhas_previous_funding: 1 indicates the block received ACAM I and/or ACAM II and 0 indicates it did not.\n\nWe produced two tables, proj_bead.acam_bl_tidy and proj_bead.acam_bl_wide.\nWhen generating the proj_bead.acam_bl_tidy table we had to crosswalk the 2010 block fips to the 2020 block fips using the proj_bead.cross_tab1020. We did this to create consistency with the other BEAD datasets.\nSome 2010 blocks with different funding flags were merged into one block in 2020. The 2020 block will be flagged as previously funded`if any of the 2010 blocks were previously funded.\nWide format table, proj_bead.acam_bl_wideis grouped by Census Block and summarized. The table is kept at 2010 blocks to avoid duplicate rows.",
    "crumbs": [
      "Home",
      "EDA",
      "High Cost: Connect America Fund Broadband Map (CAF Map)"
    ]
  },
  {
    "objectID": "ms-eda.html",
    "href": "ms-eda.html",
    "title": "Microsoft Building footprint",
    "section": "",
    "text": "Why are we doing it?\nCurrently we have FCC total count locations at census block level.",
    "crumbs": [
      "Home",
      "EDA",
      "Microsoft Building footprint"
    ]
  },
  {
    "objectID": "ms-eda.html#overview",
    "href": "ms-eda.html#overview",
    "title": "Microsoft Building footprint",
    "section": "Overview",
    "text": "Overview\nMS used satellite data (from multiple campains/vintage) to get the footprint of buildings.\nThey are classifying pixels that are supposed to be part of a buildings (segmentation using a neural network) and then convert pixels to a shape.\nIt exists worldwide and for US states.\n\nWe have processed the 51 states\nAdditional works will be required for PR and US territories (parts of workflow from the US states can be reused).\n\nThe precision of the model vary depending on the region: the Carribean is at 92,2% and the US at 98.5%. The rate of false positive is 1% for the US and 1.8% for the Carribean. (Oceania was not provided)\nThe license is ODbL.",
    "crumbs": [
      "Home",
      "EDA",
      "Microsoft Building footprint"
    ]
  },
  {
    "objectID": "ms-eda.html#buildings-to-bsl",
    "href": "ms-eda.html#buildings-to-bsl",
    "title": "Microsoft Building footprint",
    "section": "Buildings to BSL?",
    "text": "Buildings to BSL?\nBuildings are shapes, BSL are points. We converted the buildings to single point (arbitrary: the first vertex of the shape) to lower the amout of data. Hence, now we have “buildings” summarized to points (lat/long).\nWe do not have access to lat/long of BSL (fabrics). Our assumptions is if a count at block match they are describing the same reality (we can’t do the “on the ground verification”).\nThe number of buildings reported for 51 states is: 130 099 920\nWhile is the number of BSL is: 114 074 438\n\nOther potential sources:\n\nOpenStreetMap\nFCC staff estimates: at census block level (only for US 51)\nACS households",
    "crumbs": [
      "Home",
      "EDA",
      "Microsoft Building footprint"
    ]
  },
  {
    "objectID": "ms-eda.html#ms-building-footprint-in-vt",
    "href": "ms-eda.html#ms-building-footprint-in-vt",
    "title": "Microsoft Building footprint",
    "section": "MS building footprint in VT",
    "text": "MS building footprint in VT\nWe can count those points per block and compare to the number of location than FCC is descriving.\nAfter that we build a small model that will provide either an estimate of BSL given MS footprint and how confidant the model is.\n\n\nCode\nvt &lt;- read.csv(\"data/vt_ms.csv\")\nvt$tot_loc &lt;-as.numeric(vt$tot_loc) \nvt$tot_loc &lt;- ifelse(is.na(vt$tot_loc), 0, vt$tot_loc)\n\nlm_ms &lt;- lm(cnt_ms ~ tot_loc, data = vt)\npot_val &lt;- seq(min(vt$tot_loc), max(vt$tot_loc), by = 0.025)\nconf_interval &lt;- predict(lm_ms, newdata = data.frame( tot_loc = pot_val) ,\n                         interval = \"prediction\", level = 0.95)\n\nplot(vt$cnt_ms, vt$tot_loc,  col = \"#DF536B50\", asp = 1, \nxlab = \"cnt of MS buildings\", ylab = \"cnt of fcc locations\") \nabline(lm_ms, col = 4)\nabline(1:500, 1:500, col = \"black\")\nlines(pot_val, conf_interval[, \"lwr\"], col = \"blue\", lty = 2)\nlines(pot_val, conf_interval[, \"upr\"], col =\" blue\", lty = 2)\n\n\n\n\n\n\n\n\n\n\nmy model (just a linear one) is probably bad (log should correct that)\nstill strong relation\nthe model is “overconfidant”, and the reality is more “spread”\nVT MS has also more locations (285333, versus 352618)",
    "crumbs": [
      "Home",
      "EDA",
      "Microsoft Building footprint"
    ]
  },
  {
    "objectID": "fcc_funding.html",
    "href": "fcc_funding.html",
    "title": "FCC Funding Map",
    "section": "",
    "text": "Code\nsource(\"R/table_with_options.R\")\nLink: https://fundingmap.fcc.gov/home\nDocumentation: https://us-fcc.app.box.com/v/bfm-data-downloads-output\nThe download page has two tabs:",
    "crumbs": [
      "Home",
      "EDA",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "fcc_funding.html#program-data",
    "href": "fcc_funding.html#program-data",
    "title": "FCC Funding Map",
    "section": "Program Data",
    "text": "Program Data\nIt is organized by Agency and can be downloaded by projects (fundingdata_projectXXXX) or for all projects in a program (fundingdata_programXX).\nFor each download it will have a csv about the program. For example RDOF is just a two rows csv, header included.",
    "crumbs": [
      "Home",
      "EDA",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "fcc_funding.html#project-data",
    "href": "fcc_funding.html#project-data",
    "title": "FCC Funding Map",
    "section": "Project Data",
    "text": "Project Data\nFCC is defining 3 types of projects:\n\nDefined by Area\nDefined by list of locations\nDefined by Middle Mile (No project representing this one: 07-03-2024)\n\nAll of those types of project will have a Project Attribute Information table (areaattributes_program or locationattributes_program). The structure of those files are close but not similar for example location project has columns related to locations (build_req, loc_plan, loc_sup).\nThe one for RDOF (areaattributes_program24_J23_12feb2024.csv) has 474 rows (inclunding headers ie 473 projects).\nArea projects will have an associated areapolygons_XXX.gpkg. A quick glance on the one from RDOF show full valid geometries with an expected number of rows (473).\nLocation projects have, instead, of a gpkg a csv with location_id and lat/long (and address)!\nIt seems that you can also have buildout data associated which each type of project but none of the project available are providing this information(07-03-2024).",
    "crumbs": [
      "Home",
      "EDA",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "fcc_funding.html#list-of-dataset-avalaible-07-03-2024",
    "href": "fcc_funding.html#list-of-dataset-avalaible-07-03-2024",
    "title": "FCC Funding Map",
    "section": "List of dataset avalaible: 07-03-2024",
    "text": "List of dataset avalaible: 07-03-2024\n\n\nCode\nagency_name &lt;- \"Federal Communications Commission\"\nprogram_name &lt;- c(\"Bringing Puerto Rico Together\",\n                \"Connect America Fund Phase II\",\n                \"Connect USVI\", \n                \"Enhanced Alternative Connect America Cost Model\",\n                \"Rural Digital Opportunity Fund\")\nprogram_id &lt;-  c(\"25\", \"28\", \"26\", \"35\", \"24\")\nfcc_dat &lt;- data.frame(agency_name = rep(agency_name, length(program_name)),\n                     program_name,\n                     program_id \n                      )\nagency_name &lt;- \"NTIA\"\nprogram_name &lt;- c(\n  \"Broadband Infrastructure Program\",\n  \"Tribal Broadband Connectivity Program NOFO 1\"\n)\nprogram_id &lt;-  c(\"11\", \"27\")\nntia_dat &lt;- data.frame(agency_name = rep(agency_name, length(program_name)), \n                      program_name,\n                      program_id \n                      )\nagency_name &lt;- \"Rural Utilities Service\"\nprogram_name &lt;- c(\n  \"COMMUNITY CONNECT GRANT PROGRAM\",\n  \"RURAL ECONNECTIVITY PROGRAM\",\n  \"TELEPHONE LOAN PROGRAM\"\n)\nprogram_id &lt;-  c(\"10\", \"6\", \"12\")\nrural_dat &lt;- data.frame(agency_name = rep(agency_name, length(program_name)), \n                      program_name,\n                      program_id \n                      )\nagency_name &lt;- \"US Department of Treasury\"\nprogram_name &lt;- c(\n  \"Capital Projects Fund\",\n  \"State and Local Fiscal Recovery Fund\"\n)\nprogram_id &lt;-  c(\"18\", \"19\")\nusdt_dat &lt;- data.frame(agency_name = rep(agency_name, length(program_name)), \n                      program_name,\n                      program_id \n                      )\n\nfcc_all_dat &lt;- rbind(fcc_dat, ntia_dat, rural_dat, usdt_dat )\n\n# ls &gt; path/to/list_file_fcc_feb2024.txt\nfcc_files &lt;- readLines(\"data/list_file_fcc_feb2024.txt\")\n# remove zip\nfcc_files_slim &lt;- fcc_files[!grepl(\".zip\", fcc_files)]\n\nfcc_files_tidy &lt;- as.data.frame(\n                                do.call(rbind, \n                                        strsplit(fcc_files_slim, \"_\"))\n)\n# remove programdata, but it is nice to see for every files \nfcc_files_tidy &lt;- fcc_files_tidy[fcc_files_tidy[[\"V1\"]] != \"programdata\",]\n\nfcc_files_tidy[[\"program_id\"]] &lt;- gsub(\"program\", \"\", fcc_files_tidy[[\"V2\"]])\n\nfcc_files_tidy[[\"is_area\"]] &lt;- grepl(\"area\", fcc_files_tidy[[\"V1\"]])\n\nfirst_V4 &lt;- function(x) {unlist(strsplit(x, \".\", fixed = TRUE))[1]}\n\nfcc_files_tidy[[\"file_release\"]] &lt;- sapply(fcc_files_tidy[[\"V4\"]], first_V4)\n\n# works for now but will breack if I have the third type of project\nfcc_files_tidy[[\"type_proj\"]] &lt;- ifelse(fcc_files_tidy[[\"is_area\"]], \"area\", \"location\")\n\ntype_proj_temp &lt;- sapply(split(fcc_files_tidy[[\"type_proj\"]], \n             fcc_files_tidy[[\"program_id\"]]), \n             unique)\nfile_release &lt;- sapply(split(fcc_files_tidy[[\"file_release\"]], \n             fcc_files_tidy[[\"program_id\"]]), \n             unique) \n\ntype_proj &lt;- data.frame(\n  program_id = names(type_proj_temp),\n  type_proj  = type_proj_temp, \n  # a bit lazy and should be a join \n  file_release = file_release\n)\n\nfcc_all_dat &lt;- merge(fcc_all_dat, type_proj,\n      by.x = \"program_id\", by.y = \"program_id\", \n      all.x = TRUE, all.y = TRUE)\n\ntable_with_options(fcc_all_dat)",
    "crumbs": [
      "Home",
      "EDA",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "fcc_funding.html#from-fcc-program-csv",
    "href": "fcc_funding.html#from-fcc-program-csv",
    "title": "FCC Funding Map",
    "section": "From FCC program csv",
    "text": "From FCC program csv\nI just stacked them:\ncsvstack data/programdata_program* &gt; data/all_program.csv\nThen remove their elig_rules_desc and program_desc so it can fit in a table.\n\n\nCode\nall_prog &lt;- read.csv(\"data/all_program.csv\")\nlist_of_names_to_keep &lt;- c(\"agency_name\" ,  \"program_id\",\n                          \"program_start_date\",\"program_end_date\",    \"funding_source\",   \"funding_type\",        \"funding_obligated\",  \"funding_disbursed\",   \"funding_defaulted\",   \"min_download_spd\",    \"min_upload_spd\",     \"low_latency\",      \"funding_grant\",       \"program_cost\",       \"funding_loan\",        \"assistance_listings\", \"program_acronym\",     \"program_url\"  \n)\ntable_with_options(all_prog[, list_of_names_to_keep])",
    "crumbs": [
      "Home",
      "EDA",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "ntia_test.html",
    "href": "ntia_test.html",
    "title": "Testing other eligibilities",
    "section": "",
    "text": "Code\nsource(\"R/table_with_options.R\")\nsource(\"https://gist.githubusercontent.com/defuneste/32d5bffce8c6e88b6322ca4c9861793b/raw/893462fed28362601e63434ecc140bbdcbf6928b/us_states_df.R\")\nThe goal of this page is to record the effect on not including some technologies on the eligibility of BSL and census block.",
    "crumbs": [
      "Home",
      "EDA",
      "Testing other eligibilities"
    ]
  },
  {
    "objectID": "ntia_test.html#excluding-dsl-service",
    "href": "ntia_test.html#excluding-dsl-service",
    "title": "Testing other eligibilities",
    "section": "Excluding DSL service",
    "text": "Excluding DSL service\n\n\nCode\nselect\n    geoid_st,\n    sum(cnt_total_locations) as cnt_total_locations,\n    sum(cnt_underserved) as cnt_underserved,\n    sum(cnt_underserved_dsl_excluded) as cnt_loc_underserved_dsl_excluded,\n    count(*) as cnt_blocks,\n    sum(case when bl_100_20_area = 'underserved_area' then 1 else 0 end) as cnt_underserved_block,\n    sum(case when bl_100_20_area_dsl_excluded = 'underserved_area' then 1 else 0 end) as cnt_underserved_block_dsl_excluded\nfrom staging.bead_source_tiger_2020_blocks\ngroup by geoid_st\norder by geoid_st\n\n\nThis table will summarize some effects on locations and census blocks categories.\nOur previous definition:\n\nWe are excluding satellites and unlicensed wireless\nIf a location has only services lower than 25/3 it is unserved. If a location only has services with speed between 25/3 and 100/20 it is underserved. It will be served if it equal and above 100/20.\n\nNew definition:\n\nWe are now excluding DSL (technology 10).\nUnserved are not changing (if they had DSL under 25/3 it was already unserved). Underseved and Served will change because some locations who were served will now move to underserved.\n\nThis was for locations. If we move to census block (where we apply this “uncommun” 80/20) it will also change the category of eligibility (because we are changing the classification of locations).\nTable dicvtionnary:\n\n“United.States.of.America”: names of States\n“geoid_st: ANSI”number”\n“number_block”: Number of Census Block per states\n“cnt_underserved_block” : count of blocks underserved with the previous definition\n“cnt_underserved_block_dsl_excluded”: count of block if we exclude DSL\n“diff_block” : cnt_underserved_block_dsl_excluded - cnt_underserved_block\n“cnt_total_locations”: count of total locations per States\n“cnt_underserved”: count of locations underserved with previous definition\n“cnt_loc_underserved_dsl_excluded”: count of locations underserved if we removed DSL\n\nI provided the locations to:\n\ndo a bit of sanity check\nWorking at the block level imply using the 80/20 rules and kind of assum all block are the same.\n\n\n\nCode\neligibiliy_st_ntia &lt;- read.csv(\"data/dsl-exluded.csv\", colClasses =c(\"geoid_st\" = \"character\"))\nUS_slim &lt;- US_states[,c(\"United.States.of.America\", \"ANSI_num\", \"ANSI_let\")]\neasy_table &lt;- merge(eligibiliy_st_ntia, US_slim, \n                    by.x = \"geoid_st\", by.y = \"ANSI_num\",\n                    all.x = TRUE, all.y = TRUE)\n\neasy_table$diff_block &lt;- easy_table$cnt_underserved_block_dsl_excluded - easy_table$cnt_underserved_block\n\neasy_table &lt;- easy_table[, c(\"United.States.of.America\",\n                            \"geoid_st\",\n                            \"number_block\", \n                            \"cnt_underserved_block\",\n                            \"cnt_underserved_block_dsl_excluded\",\n                            \"diff_block\",\n                            \"cnt_total_locations\",\n                            \"cnt_underserved\",\n                            \"cnt_loc_underserved_dsl_excluded\"\n                            )]\n\ntable_with_options(easy_table)",
    "crumbs": [
      "Home",
      "EDA",
      "Testing other eligibilities"
    ]
  },
  {
    "objectID": "ntia_test.html#some-tldr",
    "href": "ntia_test.html#some-tldr",
    "title": "Testing other eligibilities",
    "section": "Some tl:dr:",
    "text": "Some tl:dr:\nNot keeping DSL move 26907 blocks from being served to be underserved.\nIt has heterogneous effects: for some States it has nearly or very low impacts but for other it is importants.",
    "crumbs": [
      "Home",
      "EDA",
      "Testing other eligibilities"
    ]
  },
  {
    "objectID": "isp_eda.html",
    "href": "isp_eda.html",
    "title": "EDA on ISP",
    "section": "",
    "text": "Code\nsource(\"R/table_with_options.R\")\nWe are starting a first exploratory data analysis around ISPs in the FCC BDC data set. It should be kept in mind that an ISP can be multiple time in the same location (offering multiple service).\nThe query that generated this first pass at it is here:\nThe name of the column match FCC description.\nWe are adding:\nIt can be explored here:\nCode\nisp &lt;- read.csv(\"data/isp_v2.csv\")\ncolnames(isp) &lt;- c(\"brand_name\", \"state_abbr\", \"technology\",\n\"provider_id\", \"cnt_services\", \"cnt_total_locations\", \"cnt_block_presence\")\n\nisp$temp &lt;- paste0(isp$brand_name, isp$provider_id)\nisp &lt;- isp[order(isp$temp),]\nisp$ID &lt;- cumsum(!duplicated(isp$temp))\nisp$ct[!duplicated(isp$ID)] &lt;- 1 \nisp$multiple_name_id &lt;- ave(isp$ct, isp$provider_id, FUN = function(x) sum(x , na.rm = TRUE))\nisp &lt;- isp[,c(\"ID\", \"brand_name\", \"provider_id\", \"multiple_name_id\", \"state_abbr\", \"technology\", \n            \"cnt_services\", \"cnt_total_locations\", \"cnt_block_presence\")]\ntable_with_options(isp[order(isp$cnt_services,decreasing = TRUE),])",
    "crumbs": [
      "Home",
      "EDA",
      "EDA on ISP"
    ]
  },
  {
    "objectID": "isp_eda.html#numbers-for-context",
    "href": "isp_eda.html#numbers-for-context",
    "title": "EDA on ISP",
    "section": "Numbers for context:",
    "text": "Numbers for context:\n\n\nCode\nfilter_sat &lt;- c(60, 61, 70)\nisp_slim &lt;- isp[! isp$technology %in% filter_sat, ]\n\n\nIn June 2023 we have 2902 unique ISP brand name but only 2431 if we remove the ISP that only offer Satellite or Unlicensed Wireless.\nFor the rest of the analysis I will not take into account Satellite data and Unlicensed Wireless.\nOur first step will be to try having some “unique” brand name so we can be confident we are correctly counting the same ISP (or not).",
    "crumbs": [
      "Home",
      "EDA",
      "EDA on ISP"
    ]
  },
  {
    "objectID": "isp_eda.html#organize-a-bit-brand_name-and-provider_id",
    "href": "isp_eda.html#organize-a-bit-brand_name-and-provider_id",
    "title": "EDA on ISP",
    "section": "Organize a bit brand_name and provider_id",
    "text": "Organize a bit brand_name and provider_id\n::: {.cell}\n\nCode\nisp_slim$brand_name &lt;- tolower(isp_slim$brand_name)\n\n:::\nIt seems that we have:\n- brand name with and without capital letter (VERIZON, Verizon): if we tolower brand name we get 2414 unique brand name.\n\n\nCode\nisp_agg &lt;- aggregate(isp_slim[\"cnt_services\"], isp_slim[\"brand_name\"], sum)\ntable_with_options(isp_agg[order(isp_agg$cnt_services, decreasing = TRUE), ])\n\n\n\n\n\n\nI have done a smaller .csv just with brand_name provider_id and cnt_services just to inspect what is the relation between them (1 to 1 / 1 to many). Outside of typos we should not have many to many relation.\nSELECT\n    brand_name,\n    provider_id,\n    count(*) cnt_services\nFROM staging.june23\nGROUP BY brand_name, provider_id\nORDER BY cnt_services desc;\n\n\nCode\nisp_list &lt;- read.csv(\"data/isp_prov.csv\")\nisp_list$ct &lt;- 1  \nisp_list$name_id &lt;- ave(isp_list$ct, isp_list$provider_id, FUN = sum)\n#View(isp_list[!is.na(isp_list$new_name),])\n\n\n\nTCT\n\n\nCode\ntable_with_options(isp_list[grepl(\"^TCT \", isp_list$brand_name) ,])\n\n\n\n\n\n\nCode\nisp_list[isp_list$provider_id == 410172,]\n\n\n                                     brand_name provider_id cnt_services ct\n1584 The Tri-County Telephone Association, Inc.      410172         4260  1\n1957            Council Grove Telephone Company      410172         1992  1\n2025                                        TCT      410172         1681  1\n     name_id\n1584       3\n1957       3\n2025       3\n\n\nTCT has some non-conventional names but nearly all of them has the same provider_id. An other “TCT” exist but with a different provider_id (410172) shared with two other brand name. I will assume that all of this TCT XXX are the same and provide them with a temporary name TCT_131366.\n\n\nCode\nisp_list$new_name[grepl(\"^TCT \", isp_list$brand_name)] &lt;- \"TCT_131366\"\n\n\n\n\nWindstream\nWindtream present a similar case but the position of Windstream is not always the first word (Georgia Windstream, LLC). I went with the solution than TCT: Windstream_131413\n\n\nCode\ntable_with_options(isp_list[grepl(\"Windstream\", isp_list$brand_name) ,]) \n\n\n\n\n\n\nCode\nisp_list$new_name[grepl(\"Windstream\", isp_list$brand_name)] &lt;- \"Windstream_131413\" \n\n\n\n\nAcentek/Acentek\nIt exists in both forms (tolower() will correct it) but it is also sharing it’s provider_id with some non-conventional “name”:\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 130008,])\n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 130008] &lt;- \"acentek\"\n\n\nFor now I will go with attributing them to “acentek”` but an other option will be to just remove them.\n\n\nMediacom - Bolt\n\n\nCode\ntable_with_options(isp_list[grepl(\"Mediacom|Bolt\", isp_list$brand_name) ,]) \n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 130804] &lt;- \"mediacom_bolt\"\n\n\nIt appears that Bolt and Mediacom share the same provider_id and are together in some brand_name. I think we should regroup them but this definietly more domain knowledge than the one I have!\n\n\nComporium\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 131125,])\n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 131125] &lt;- \"comporium\"\n\n\nFor this one I am for renaming them “comporium”\n\n\nArmstrong\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 130071,])\n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 130071] &lt;- \"armstrong\"\n\n\nIdem label to “armstrong”?\n\n\nTEC\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 131311,])\n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 131311] &lt;- \"TEC\"\n\n\n\n\nPUD\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 290075,])\n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 290075] &lt;- \"PUD\"\n\n\nLabel to “PUD” ?\n\n\nGoNetspeed?\nI am unsure about that one. We can regroup the two GoNetspeed but we are lacking information for the rest.\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 131378,])\n\n\n\n\n\n\n\n\nMHTC\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 130862,])\n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 130862] &lt;- \"MHTC\"\n\n\nLabel MHTC?\n\n\nHardy\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 130588,])\n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 130588] &lt;- \"Hardy\"\n\n\nlabel Hardy?\n\n\nOmniTel\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 130484,])\n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 130484] &lt;- \"OmniTel\"\n\n\n\n\nHamilton\n\n\nCode\ntable_with_options(isp_list[isp_list$provider_id == 130887,])\n\n\n\n\n\n\nCode\nisp_list$new_name[isp_list$provider_id == 130887] &lt;- \"Hamilton\"\n\n\nlabel Hamilton?\n\n\nMultiple brand_name with same provider_id\n\nprovider_id: 131413, same provider id than windstream but probably a different entity?\nprovider_id 130485: look very similar (SHLB in the name 8/10)\nlist of provider_id associated with multiple brand name:\n\n130074\n131378\n190233\n160127\n150277\n150266\n130183\n300192\n131362\n130877\n130757 (regroup Long lines?)\n130778 (Manti 5/6)\n330025\n130254 (2/5 altafiber)\n150334\n130906\n130425 (2/5 Lavaca)\n130206\n130453 (3/5 EFIBER)\n140092 (3/5 Twin Valley)\n140030\n130142",
    "crumbs": [
      "Home",
      "EDA",
      "EDA on ISP"
    ]
  },
  {
    "objectID": "isp_eda.html#todo-list",
    "href": "isp_eda.html#todo-list",
    "title": "EDA on ISP",
    "section": "TODO list:",
    "text": "TODO list:\n[ ] provider_id: 586211",
    "crumbs": [
      "Home",
      "EDA",
      "EDA on ISP"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FCC workflow",
    "section": "",
    "text": "We are using two sources of data from FCC:\n\nNational Broadband Map (NBM)\nBroadband Funding Map (BFM)\n\n\nbold\n\nitalic\n\nbob &lt;- 1+2\nbob * 2\n\n[1] 6\n\n\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "road-eda.html",
    "href": "road-eda.html",
    "title": "TIGER Roads",
    "section": "",
    "text": "Requirement:",
    "crumbs": [
      "Home",
      "EDA",
      "TIGER Roads"
    ]
  },
  {
    "objectID": "road-eda.html#what-is-inside-the-data",
    "href": "road-eda.html#what-is-inside-the-data",
    "title": "TIGER Roads",
    "section": "what is inside the data:",
    "text": "what is inside the data:\nWe have linestrings with those attributes:\n\nlinearid: primary id, used by ither TIGER product\nfullname: road name, humane-readable\nrttyp: route type code, type of road.\n\n\n\n\nRoute Type Code\nRoute Type Code Description\n\n\n\n\nC\nCounty\n\n\nI\nInterstate\n\n\nM\nCommon Name\n\n\nO\nOther\n\n\nS\nState recognized\n\n\nU\nU.S.\n\n\n\n\nmtfcc: MAF/TIGER Feature Class Codes, example, S1400 (Local Neighborhood Road, Rural Road, City Street)\n\n\nClasses of Roads according to TIGER/line census\n\n\nTIGER: Topologically Integrated Geographic Encoding and Referencing\n\n\n\n\n\n\n\nmtfcc\nFeatures\n\n\n\n\n“S1100”\nPrimary Road\n\n\n“S1200”\nSecondary Road\n\n\n“S1400”\nLocal Neighborhood Road, Rural Road, City Street\n\n\n“S1500”\nVehicular Trail (4WD)\n\n\n“S1630”\nRamp\n\n\n“S1640”\nService Drive usually along a limited access highway\n\n\n“S1710”\nWalkway/Pedestrian Trail\n\n\n“S1720”\nStairway\n\n\n“S1730”\nAlley\n\n\n“S1740”\nPrivate Road for service vehicles (logging, oil fields, ranches, etc.)\n\n\n“S1750”\nInternal U.S. Census Bureau use\n\n\n“S1780”\nParking Lot Road\n\n\n“S1810”\nWinter Trail\n\n\n“S1820”\nBike Path or Trail\n\n\n“S1830”\nBridle Path",
    "crumbs": [
      "Home",
      "EDA",
      "TIGER Roads"
    ]
  },
  {
    "objectID": "road-eda.html#should-this-information-provided-and-how",
    "href": "road-eda.html#should-this-information-provided-and-how",
    "title": "TIGER Roads",
    "section": "Should this information provided and how?",
    "text": "Should this information provided and how?\nWe should probably not keep the bridle paths?\nShould it be provided at block level: ie an estimate route miles?\nHow the user will interact with it?\n\nResources\n\nhttps://www2.census.gov/geo/pdfs/reference/mtfccs2020.pdf\nhttps://www.census.gov/library/reference/code-lists/route-type-codes.html",
    "crumbs": [
      "Home",
      "EDA",
      "TIGER Roads"
    ]
  }
]