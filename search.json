[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FCC Primer for CORI’s MDA",
    "section": "",
    "text": "We are using two sources of data from the FCC (Federal Communications Commission):\nTheir license can be found here and here, respectively:"
  },
  {
    "objectID": "index.html#what-are-these-two-datasets",
    "href": "index.html#what-are-these-two-datasets",
    "title": "FCC Primer for CORI’s MDA",
    "section": "What are these two datasets?",
    "text": "What are these two datasets?\nThe first one was started by the FCC in November 20221 while the second was first published in May 20232 (and documentation3 in July 2023).\nThe NBM provides information at the scale of a “service” - a location covered by a provider by a technology with specifics maximum speeds.\nEvery location is characterized by:\n\nWho is providing those services (frn, provider_id, and brand_name)\nA description of each services (technology, max_advertised_download_speed, max_advertised_upload_speed, low_latency)\nWhether the location associated with residential, business or both\nways to localize the location (state_abbr, block_geoid, h3_res8_id)\n\n\n\n\n\n\n\nTip\n\n\n\nA location (see Section 2.1) can be covered by multiple Internet Services Provides (ISP) with potentially different services and technologies. Hence, it can represented in the data can by many “rows”.\n\n\nWe are using the “Broadband Availability” dataset (see Figure 1, below) that comes from the “Fabric” locations (developed by CostQuest) and provides the basis of the National Broadband Map. The locations are determined within the Fabric locations data.\n\n\nSometimes the process of collecting those two datasets is called Broadband Data Collection (BDC)\nThe exact coordinates of every locations is only part of the Fabric data and within the Broadband Availability we can only link a record for a location to a Census Block (2020 vintage) or H3 hexagon.\n\n\n\n\n\n\nFigure 1: “What on the national broadband map” Source: https://www.fcc.gov/BroadbandData\n\n\n\nThe BFM provides information about “broadband infrastructure deployment projects funded by the Federal government throughout the United States”. The information is structured either at the scale of a specific project inside a program or for the whole program. Hence we have characteristics of each projects with their associated boundaries (territories covered) (see link https://ruralinnovation.github.io/proj-fcc-report/fcc_funding.html).\n\nWhat is a Broadband Service Location (BSL)?\n\nA broadband serviceable location is defined as “a business or residential location in the United States at which fixed broadband Internet access service is, or can be, installed.” A residential BSL includes all residential structures, including structures that are (or contain) housing units or group quarters (as those terms are defined by the United States Census Bureau). A business BSL includes “all non-residential (business, government, non-profit, etc.) structures that are on property without residential locations and that would expect to demand Internet access service.” (source FCC4)\n\n\n\nWhen is this data updated?\nNBM has two big releases per year (June and December) and have “unofficial” versions every two weeks to take into account challenges5. Experience has told us that sometimes their release can be faster (more than one per week) or slower. The FCC did not (April 2024) provides a changelog between releases or versions (but the documentation has some of the major changes6 ).\nBFM seems to follow a schedule of update every two weeks but we have not find any specifications.\n\n\nWhat is the geographic coverage of these datasets?\nThe Broadband Availability data is covering all US States, Puerto Rico and the US territories.\nThe coverage of the funding map depend on each specific program."
  },
  {
    "objectID": "index.html#what-is-unserved-v.s.-underseved",
    "href": "index.html#what-is-unserved-v.s.-underseved",
    "title": "FCC Primer for CORI’s MDA",
    "section": "What is unserved v.s. underseved?",
    "text": "What is unserved v.s. underseved?\nServed, Unserved and Underseved are overlapping categories at the location level. They can be extended at the “area level”.\nIf all available internet services at a location have advertised (reported) maximum speeds that are below 25 Mbps downstream speed or below 3 Mbps upstream (25/3 to simplify), then that location is categorized as unserved. If a location has at least one service with maximum speeds that are equal to or above 25/3, but no service with maximum speeds that are equal to or above 100/20, it is categorized as underserved. If a location has at least one service with maximum speeds that are equal to or above 100/20, then that location is categorized as served.\nThose definitions are recommended in the FCC’s Broadband Speed Benchmark and can be adapted by every States7."
  },
  {
    "objectID": "index.html#our-products",
    "href": "index.html#our-products",
    "title": "FCC Primer for CORI’s MDA",
    "section": "Our products:",
    "text": "Our products:\nTODO: improve\n\nBroadband Climate Risk Mitigation Tool\nInteractive Rural Broadband Map,\nHelping our Broadband team\nHelping our Research team"
  },
  {
    "objectID": "index.html#quick-facts-may-2024",
    "href": "index.html#quick-facts-may-2024",
    "title": "FCC Primer for CORI’s MDA",
    "section": "Quick facts, May 2024:",
    "text": "Quick facts, May 2024:\n\nBroadband Availibility:\n\nNumber of BSL: 115 342 225\nNumber of unique FRN: 2879\n\n\n\nFunding Map:\n\n4 agencies are contributing\nThe Funding Map consist of 12 programs (1853 projects) with two specific to Puerto Rico (PR) and US territories.\nThose projects are classified by FCC in 3 categories: “Area”, “List of locations” and “Midle mile”8"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "FCC Primer for CORI’s MDA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.fcc.gov/news-events/notes/2022/11/18/new-broadband-maps-are-finally-here↩︎\nhttps://docs.fcc.gov/public/attachments/DA-23-410A1.pdf↩︎\nSee Changelog https://us-fcc.app.box.com/v/bfm-data-downloads-output↩︎\n“The Fabric data”↩︎\nhttps://www.fcc.gov/sites/default/files/bdc-challenge-overview.pdf↩︎\nSee “Change Log” https://us-fcc.app.box.com/v/bdc-data-downloads-output↩︎\nPage 4 https://www.pewtrusts.org/-/media/assets/2023/06/un–and-underserved-definitions-ta-memo-pdf.pdf↩︎\nIn May 2024 no Midle mile were present↩︎"
  },
  {
    "objectID": "FCC_provider_list.html",
    "href": "FCC_provider_list.html",
    "title": "FCC providers list",
    "section": "",
    "text": "Code\nsource(\"R/table_with_options.R\")\nWe have multiple sources from FCC to define a provider.",
    "crumbs": [
      "Home",
      "FCC Data",
      "FCC providers list"
    ]
  },
  {
    "objectID": "FCC_provider_list.html#broadband-data-collection-versions",
    "href": "FCC_provider_list.html#broadband-data-collection-versions",
    "title": "FCC providers list",
    "section": "Broadband Data Collection versions:",
    "text": "Broadband Data Collection versions:\n\n\nCode\n#|label: read csv\n\ncsv_path &lt;- \"data/FCC_Providers_11_03_2024.csv\"\n\ncol_classe &lt;- c(\"Provider.Name\" = \"character\", \n               \"Affiliation\" = \"character\",\n               \"Operation.Type\" = \"factor\",\n               \"FRN\" = \"character\",\n               \"Provider.ID\" = \"character\")\n\nisp &lt;- read.csv(csv_path, colClasses = col_classe)\n\n\nAs we can see this table have 4 341 rows and 5 columns.\nThose columns are:\n\nProvider.Name: Sam than Brand Name?\nAffiliation: Same number than Provider.ID\n\n\n\nCode\nop_type &lt;-as.data.frame(table(isp$Operation.Type, dnn = \"Type\"), responseName = \"Nb.\" )\n\nknitr::kable(op_type)\n\n\n\n\n\n\nType\nNb.\n\n\n\n\nILEC\n973\n\n\nNon-ILEC\n3368\n\n\n\n\n\nOperation.Type: Only two options “ILEC” or “Non-ILEC”\nFRN: FCC Registration Number; “number of the entity that submited the data”. It is supposed to be a string of 10 characters (with padding 0). Slighly more number than Provider.Name and seems to be the primary key.\nProvider.ID: An ID for Affiliations\n\n\n\nCode\n#|label: display ISP from BDC\ntable_with_options(isp)",
    "crumbs": [
      "Home",
      "FCC Data",
      "FCC providers list"
    ]
  },
  {
    "objectID": "FCC_provider_list.html#footnotes",
    "href": "FCC_provider_list.html#footnotes",
    "title": "FCC providers list",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Emergency Broadband Benefit is the successor of the Affordable Connectivity Program, source: https://www.fcc.gov/broadbandbenefit↩︎",
    "crumbs": [
      "Home",
      "FCC Data",
      "FCC providers list"
    ]
  },
  {
    "objectID": "isp_eda.html",
    "href": "isp_eda.html",
    "title": "EDA on Internet Service Providers",
    "section": "",
    "text": "Code\nsource(\"R/table_with_options.R\")\nWe are starting our first exploratory data analysis around ISPs in the FCC NBM data set. It should be kept in mind that an ISP can be listed multiple times at the same location (offering multiple service).\nOur goal is being able to take FCC data and 1. correctly identify the single ISP that is providing each reported service (i.e., deduplication of ISP names) 2. correctly identify that ISP over time (from the same program and from other FCC products).\nHow should we define an ISP? How can we define coverage (should a service 0/0 be considered as part of the extent of an ISP’s coverage)?\nWe shifted a bit from exploring to trying to classify the quality of information we have from FCC about ISP.\nThe query that generated the data set is here:\nThe name of the columns match the FCC’s documented descriptions.\nWe are adding:\nWe have from FCC:\nCode\nget_me_isp &lt;- function(path) {\n  isp &lt;- read.csv(path,\n                  colClasses = c(frn =\"character\",\n                                provider_id = \"character\",\n                                brand_name = \"character\",\n                                cnt_locations = \"character\",\n                                cnt_locations_services = \"character\",\n                                has_copperwire = \"logical\",\n                                has_coaxial_cable = \"logical\",\n                                has_fiber = \"logical\",\n                                has_wireless = \"logical\",\n                                has_satel = \"logical\",\n                                array_agg = \"character\"))\n\n  isp[[\"cnt_locations\"]] &lt;- as.numeric(isp[[\"cnt_locations\"]])                               \n  isp[[\"cnt_locations_services\"]] &lt;- as.numeric(isp[[\"cnt_locations_services\"]])  \nreturn(isp)\n}\n\nisp &lt;- get_me_isp(\"data/isp.csv\")\nCode\ntable_with_options(isp)",
    "crumbs": [
      "Home",
      "FCC Data",
      "EDA on Internet Service Providers"
    ]
  },
  {
    "objectID": "isp_eda.html#numbers-for-context",
    "href": "isp_eda.html#numbers-for-context",
    "title": "EDA on Internet Service Providers",
    "section": "Numbers for context:",
    "text": "Numbers for context:\n\nRaw numbers out of the box:\nNumber of unique frn: 2879\nNumber of unique provider_id: 2184\nNumber of unique brand name pre cleaning: 2902\n\n\nCode\ncount_and_clean &lt;- function(vec) {\n  length(unique(tolower(trimws(gsub(\"_\", \" \", vec)))))} \n\nnum_brand_name &lt;- count_and_clean(isp[[\"brand_name\"]])\n\n\nRemoving all capitalization and change underscore for white space help lower the number of unique brand names to: 2878\n\n\nCode\nisp[[\"clean_name\"]] &lt;- tolower(trimws(gsub(\"_\", \" \", isp[[\"brand_name\"]]))) \n\n\n\n\nPotential sources of errors:\n\nFRN can be wrong or not meaningfull\nprovider_id can be wrong\nbrand names can be different or evolve over time.\n\nOne case:\n\nFirst example different FRN and Provider ID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrn\nprovider_id\nbrand_name\ncnt_locations\ncnt_locations_services\nhas_copperwire\nhas_coaxial_cable\nhas_fiber\nhas_wireless\nhas_satel\narray_agg\n\n\n\n\n\n\n0003738655\n130432\n“EATEL Corp.”\n83537\n86548\ntrue\ntrue\ntrue\nfalse\nfalse\n{LA}\n\n\n\n\n0009873712\n131103\n“EATEL Corp.”\n34494\n34497\nfalse\ntrue\ntrue\nfalse\nfalse\n{LA}\n\n\n\n\n\nOther case:\n\nAcentek here have the same provider ID but different FRN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrn\nprovider_id\nbrand_name\ncnt_locations\ncnt_locations_services\nhas_copperwire\nhas_coaxial_cable\nhas_fiber\nhas_wireless\nhas_satel\narray_agg\n\n\n\n\n\n\n0002626984\n130008\nAcentek\n47\n47\ntrue\nfalse\ntrue\nfalse\nfalse\n{MN}\n\n\n\n\n0002626984\n130008\nACENTEK\n1395\n1395\ntrue\nfalse\nfalse\nfalse\nfalse\n{MN}\n\n\n\n\n0002645927\n130008\nAcentek\n19521\n26636\ntrue\nfalse\ntrue\ntrue\nfalse\n{IA,MN}\n\n\n\n\n\n\n\nRules for problems:\n\nLess than 10 locations per rows:\n\n\nCode\nisp[[\"few_locations\"]] &lt;- NA_integer_\n\nfor (i in 1:10) {\n  isp[isp[[\"cnt_locations\"]] == i, \"few_locations\"] &lt;- i\n}\n\n#feel bad relying on table removing NA\nfew_loc &lt;- as.data.frame(table(isp$few_locations))\nnames(few_loc) &lt;- c(\"Number of locations\", \"Number of cases\")\n\nknitr::kable(few_loc)\n\n\n\nRow with less than 10 locations\n\n\nNumber of locations\nNumber of cases\n\n\n\n\n1\n43\n\n\n2\n9\n\n\n3\n12\n\n\n4\n5\n\n\n5\n2\n\n\n6\n4\n\n\n7\n8\n\n\n8\n5\n\n\n9\n3\n\n\n10\n3\n\n\n\n\n\nPotential solutions:\n\nWe can decide to not keep those rows\nMerge them with either other rows that is matching provider_id or frn (when this is an option)\n\n\n\nMore than one frn for a provider_id\n\n\nCode\ntemp &lt;- sapply(split(isp$frn, isp$provider_id), function(x) length(unique(x)))\n\ndta &lt;- data.frame(provider_id = names(temp), frn_by_provider_id = temp)\n\n# correct lower/upper case / replace underscore by  \" \"  / some white space on both side \n# triming whispace did not improve for this dataset but I should keept doing it\ndta[[\"unique_brand_name_by_provider_id\"]] &lt;-  sapply(split(isp$brand_name, isp$provider_id),\n                                      count_and_clean)\ndta[[\"same_state_by_provider_id\"]] &lt;- sapply(split(isp$array_agg, isp$provider_id), function(x) length(unique(x))) \n\nisp &lt;- merge(isp, dta, by.x = \"provider_id\", by.y = \"provider_id\",\n             all.x = TRUE, all.y = TRUE)\nmore_frn_than_provider &lt;- subset(isp, isp$frn_by_provider_id  &gt; 1)\n\ntable_with_options(more_frn_than_provider)\n\n\n\n\n\n\nUnique provider_id + brand_name are kind of “green” (for one time frame):\nNumber of green isp: 1972\nWe can have one provider_id with multiple frn and same or not brand_name (see TSC for example / 150266)\nIt seems:\n\nWindstream has 37 different frn: we can maybe test if it has windstream in it’s name ..\nOtelco/GoNet (18 cases)\nRally Networks/Oregon Telephone Company, is their frn wrongs ? (17 cases)\nSame Provider for differents frn and brand_name in Minesota (MN) (16 cases)\n160127 I do not see any kind of specific pattern for this one\n131486 seems to be RiverStreet Networks with various frn (13 cases) -&gt; will be catch by unique_brand_name_by_provider_id\n190233 multiple brand name and frn but seems to be in Texas and Oklahoma (13)\n131226 seems to be Fastwyre Broadband divided by technology and states (12 cases) -&gt; will be catch by unique_brand_name_by_provider_id\n130804 seems to be Mediacom (+ Bolt) with different states and names indicating their states (11 cases)\nGoogle Fiber (240041) seems to be have frn split by states (with a weird ‘Webpass, Inc.’) (11 cases) -&gt; will be catch by unique_brand_name_by_provider_id (except Webpass, Inc which is weird, technology is 70 that I should correct)\nAT&T Inc (130077) multiple frn (10 cases) -&gt; filter by unique_brand_name_by_provider_id\n130079 = Astound_Broadband (10 cases) -&gt; will be catch by unique_brand_name_by_provider_id\nVerizon -&gt; filter by unique_brand_name_by_provider_id\nlong ling (130757 ) & co are problematics (multiples names / one provider / 3 states )\n130906 is also hard to fix\n\nAssuming that same name (clean version) + same provider_id provide us with a unique ISP, it helps move to greensih:\n\n\nCode\ntable_with_options(more_frn_than_provider[more_frn_than_provider$unique_brand_name_by_provider_id == 1,])\n\n\n\n\n\n\nThis is removing 194 out of 1170.\n\n\nSame provider_id and same states\nNot too sure about this one.\n\n\nMore than one provider_id for a cleaned brand name\nThis is the case for “EATEL”.\n\n\nCode\ntemp &lt;- sapply(split(isp$provider_id, isp$clean_name),\n         function(x) length(unique(x)))\ndta &lt;- data.frame(clean_name = names(temp), provider_id_by_clean_name = temp)\nisp &lt;- merge(isp, dta, by.x = \"clean_name\", by.y = \"clean_name\",\n             all.x = TRUE, all.y = TRUE)\n\nprovider_id_by_clean_name &lt;- subset(isp, isp$provider_id_by_clean_name  &gt; 1)\n\ntable_with_options(provider_id_by_clean_name)\n\n\n\n\n\n\nThis could also move 63 cases in the greenish spot. -&gt; nop",
    "crumbs": [
      "Home",
      "FCC Data",
      "EDA on Internet Service Providers"
    ]
  },
  {
    "objectID": "isp_eda.html#what-are-our-optionsnext-steps",
    "href": "isp_eda.html#what-are-our-optionsnext-steps",
    "title": "EDA on Internet Service Providers",
    "section": "What are our options/next steps:",
    "text": "What are our options/next steps:\n\nMake a column “ready to go”\n\n\n\nCode\n# if it has unique brand name and frn by provider id id should be good\ntemp &lt;- isp$frn_by_provider_id + isp$unique_brand_name_by_provider_id\nisp[[\"rdy_to_go\"]] &lt;- ifelse(temp == 2, \"green\", \"not green\")\n\n# cases where we have a unique frn and provider id but not unique brand name \n# default of olive is that we need to pick a name out of more than one\ntemp &lt;- ifelse(isp$frn_by_provider_id == 1, 1, 0) + ifelse(isp$unique_brand_name_by_provider_id &gt; 1, 1, 0)\nisp[temp == 2, \"rdy_to_go\"] &lt;- \"olive\"\n\n# the few locations should be \"red\" and maybe dropped later \nisp[which(isp$few_locations == \"few locations\"), \"rdy_to_go\"] &lt;- \"red\"\n\ntable_with_options(isp)\n\n\n\n\n\n\nA good example could be 131167 and how we can discriminate Orbitel communications. We can also prob. raise the bar of “few locations”.\nA quick summary of where we are:\n\n\nCode\ntable(isp[[\"rdy_to_go\"]])\n\n\n\n    green not green     olive \n     1778      1170       222 \n\n\n\nMake an id &lt;—&gt; provider_id / frn / brand_name table",
    "crumbs": [
      "Home",
      "FCC Data",
      "EDA on Internet Service Providers"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sharing quick EDA about FCC data.\nTheir link: https://www.fcc.gov/BroadbandData\nFCC BDC: June 2023 release, downloaded 21-11-2023"
  },
  {
    "objectID": "hubb.html",
    "href": "hubb.html",
    "title": "High Cost: Connect America Fund Broadband Map (CAF Map)",
    "section": "",
    "text": "This dataset serves as the foundation for the Connect America Fund Broadband Map (CAF Map), which displays the geographic areas eligible for CAF support, as well as the specific fixed locations where carriers participating in the program have built out broadband service. The information in the CAF Map comes directly from carriers, which submit broadband deployment data annually through USAC’s High Cost Universal Broadband (HUBB) portal. The current CAF Map is based on data certified in the HUBB as of September 30, 2023. USAC independently verifies deployment to a random sample of reported locations each year to monitor carrier compliance with CAF build-out obligations, but not all data in the map has been subject to this review.\nHUBB\nSource Data\nAbout Page\n\n\n\nRecords represent a single address with awards deployed or will deploy in this year. The locations have a deployed date spanning from 2000-01-01 to 2023-09-19.\nA single address may have multiple households. This is represented by the column “Locations Deployed”.\nAvailable Fund Types are listed below but for the purpose of this request we filtered to ACAM I and ACAM II fund type.\nFund Type\n\nACAM\nACAM II\nAK Plan\nCAF II\nCAF II Auc\nCAF-BLS\nPR Fixed\nRBE\nRDOF\n\nTo determine the census block we used the field, Census Block, 2010 Census block of the deployment location.\nWe created 3 different flags:\n\nacam_i_flag: 1 indicates the block received ACAM I and 0 indicates it did not.\nacam_ii_flag: 1 indicates the block received ACAM II and 0 indicates it did not.\nhas_previous_funding: 1 indicates the block received ACAM I and/or ACAM II and 0 indicates it did not.\n\nWe produced two tables, proj_bead.acam_bl_tidy and proj_bead.acam_bl_wide.\nWhen generating the proj_bead.acam_bl_tidy table we had to crosswalk the 2010 block fips to the 2020 block fips using the proj_bead.cross_tab1020. We did this to create consistency with the other BEAD datasets.\nSome 2010 blocks with different funding flags were merged into one block in 2020. The 2020 block will be flagged as previously funded`if any of the 2010 blocks were previously funded.\nWide format table, proj_bead.acam_bl_wideis grouped by Census Block and summarized. The table is kept at 2010 blocks to avoid duplicate rows."
  },
  {
    "objectID": "hubb.html#acam-i-and-acam-ii",
    "href": "hubb.html#acam-i-and-acam-ii",
    "title": "High Cost: Connect America Fund Broadband Map (CAF Map)",
    "section": "",
    "text": "This dataset serves as the foundation for the Connect America Fund Broadband Map (CAF Map), which displays the geographic areas eligible for CAF support, as well as the specific fixed locations where carriers participating in the program have built out broadband service. The information in the CAF Map comes directly from carriers, which submit broadband deployment data annually through USAC’s High Cost Universal Broadband (HUBB) portal. The current CAF Map is based on data certified in the HUBB as of September 30, 2023. USAC independently verifies deployment to a random sample of reported locations each year to monitor carrier compliance with CAF build-out obligations, but not all data in the map has been subject to this review.\nHUBB\nSource Data\nAbout Page\n\n\n\nRecords represent a single address with awards deployed or will deploy in this year. The locations have a deployed date spanning from 2000-01-01 to 2023-09-19.\nA single address may have multiple households. This is represented by the column “Locations Deployed”.\nAvailable Fund Types are listed below but for the purpose of this request we filtered to ACAM I and ACAM II fund type.\nFund Type\n\nACAM\nACAM II\nAK Plan\nCAF II\nCAF II Auc\nCAF-BLS\nPR Fixed\nRBE\nRDOF\n\nTo determine the census block we used the field, Census Block, 2010 Census block of the deployment location.\nWe created 3 different flags:\n\nacam_i_flag: 1 indicates the block received ACAM I and 0 indicates it did not.\nacam_ii_flag: 1 indicates the block received ACAM II and 0 indicates it did not.\nhas_previous_funding: 1 indicates the block received ACAM I and/or ACAM II and 0 indicates it did not.\n\nWe produced two tables, proj_bead.acam_bl_tidy and proj_bead.acam_bl_wide.\nWhen generating the proj_bead.acam_bl_tidy table we had to crosswalk the 2010 block fips to the 2020 block fips using the proj_bead.cross_tab1020. We did this to create consistency with the other BEAD datasets.\nSome 2010 blocks with different funding flags were merged into one block in 2020. The 2020 block will be flagged as previously funded`if any of the 2010 blocks were previously funded.\nWide format table, proj_bead.acam_bl_wideis grouped by Census Block and summarized. The table is kept at 2010 blocks to avoid duplicate rows."
  },
  {
    "objectID": "fcc_funding.html",
    "href": "fcc_funding.html",
    "title": "FCC Funding Map",
    "section": "",
    "text": "Code\nsource(\"R/table_with_options.R\")",
    "crumbs": [
      "Home",
      "FCC Data",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "fcc_funding.html#important-links",
    "href": "fcc_funding.html#important-links",
    "title": "FCC Funding Map",
    "section": "Important links:",
    "text": "Important links:\nLink: https://fundingmap.fcc.gov/home\nDocumentation: https://us-fcc.app.box.com/v/bfm-data-downloads-output\nThe download page has two tabs:\n\nFunding data\nUnserved/unfunded",
    "crumbs": [
      "Home",
      "FCC Data",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "fcc_funding.html#program-data",
    "href": "fcc_funding.html#program-data",
    "title": "FCC Funding Map",
    "section": "Program Data",
    "text": "Program Data\nIt is organized by Agency and can be downloaded by projects (fundingdata_projectXXXX) or for all projects in a program (fundingdata_programXX).\nFor each download it will have a csv about the program. For example RDOF is just a two rows csv, header included.",
    "crumbs": [
      "Home",
      "FCC Data",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "fcc_funding.html#project-data",
    "href": "fcc_funding.html#project-data",
    "title": "FCC Funding Map",
    "section": "Project Data",
    "text": "Project Data\nFCC is defining 3 types of projects:\n\nDefined by “Area”\nDefined by “List of locations”\nDefined by “Middle Mile” (No project representing this one: 02-05-2024)\n\nAll of those types of project will have a Project Attribute Information table (areaattributes_program or locationattributes_program). The structure of those files are close but not similar for example location project has columns related to locations (build_req, loc_plan, loc_sup).\nThe one for RDOF (areaattributes_program24_J23_12feb2024.csv) has 474 rows (inclunding headers ie 473 projects).\nArea projects will have an associated areapolygons_XXX.gpkg. A quick glance on the one from RDOF show full valid geometries with an expected number of rows (473).\nLocation projects have, instead, of a gpkg a csv with location_id, lat/long and addresses field (but those last those are not filled).",
    "crumbs": [
      "Home",
      "FCC Data",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "fcc_funding.html#list-of-dataset-avalaible-07-03-2024",
    "href": "fcc_funding.html#list-of-dataset-avalaible-07-03-2024",
    "title": "FCC Funding Map",
    "section": "List of dataset avalaible: 07-03-2024",
    "text": "List of dataset avalaible: 07-03-2024\n\n\nCode\nagency_name &lt;- \"Federal Communications Commission\"\nprogram_name &lt;- c(\"Bringing Puerto Rico Together\",\n                \"Connect America Fund Phase II\",\n                \"Connect USVI\", \n                \"Enhanced Alternative Connect America Cost Model\",\n                \"Rural Digital Opportunity Fund\")\nprogram_id &lt;-  c(\"25\", \"28\", \"26\", \"35\", \"24\")\nfcc_dat &lt;- data.frame(agency_name = rep(agency_name, length(program_name)),\n                     program_name,\n                     program_id \n                      )\nagency_name &lt;- \"NTIA\"\nprogram_name &lt;- c(\n  \"Broadband Infrastructure Program\",\n  \"Tribal Broadband Connectivity Program NOFO 1\"\n)\nprogram_id &lt;-  c(\"11\", \"27\")\nntia_dat &lt;- data.frame(agency_name = rep(agency_name, length(program_name)), \n                      program_name,\n                      program_id \n                      )\nagency_name &lt;- \"Rural Utilities Service\"\nprogram_name &lt;- c(\n  \"COMMUNITY CONNECT GRANT PROGRAM\",\n  \"RURAL ECONNECTIVITY PROGRAM\",\n  \"TELEPHONE LOAN PROGRAM\"\n)\nprogram_id &lt;-  c(\"10\", \"6\", \"12\")\nrural_dat &lt;- data.frame(agency_name = rep(agency_name, length(program_name)), \n                      program_name,\n                      program_id \n                      )\nagency_name &lt;- \"US Department of Treasury\"\nprogram_name &lt;- c(\n  \"Capital Projects Fund\",\n  \"State and Local Fiscal Recovery Fund\"\n)\nprogram_id &lt;-  c(\"18\", \"19\")\nusdt_dat &lt;- data.frame(agency_name = rep(agency_name, length(program_name)), \n                      program_name,\n                      program_id \n                      )\n\nfcc_all_dat &lt;- rbind(fcc_dat, ntia_dat, rural_dat, usdt_dat )\n\n# ls &gt; path/to/list_file_fcc_feb2024.txt\nfcc_files &lt;- readLines(\"data/list_file_fcc_feb2024.txt\")\n# remove zip\nfcc_files_slim &lt;- fcc_files[!grepl(\".zip\", fcc_files)]\n\nfcc_files_tidy &lt;- as.data.frame(\n                                do.call(rbind, \n                                        strsplit(fcc_files_slim, \"_\"))\n)\n# remove programdata, but it is nice to see for every files \nfcc_files_tidy &lt;- fcc_files_tidy[fcc_files_tidy[[\"V1\"]] != \"programdata\",]\n\nfcc_files_tidy[[\"program_id\"]] &lt;- gsub(\"program\", \"\", fcc_files_tidy[[\"V2\"]])\n\nfcc_files_tidy[[\"is_area\"]] &lt;- grepl(\"area\", fcc_files_tidy[[\"V1\"]])\n\nfirst_V4 &lt;- function(x) {unlist(strsplit(x, \".\", fixed = TRUE))[1]}\n\nfcc_files_tidy[[\"file_release\"]] &lt;- sapply(fcc_files_tidy[[\"V4\"]], first_V4)\n\n# works for now but will breack if I have the third type of project\nfcc_files_tidy[[\"type_proj\"]] &lt;- ifelse(fcc_files_tidy[[\"is_area\"]], \"area\", \"location\")\n\ntype_proj_temp &lt;- sapply(split(fcc_files_tidy[[\"type_proj\"]], \n             fcc_files_tidy[[\"program_id\"]]), \n             unique)\nfile_release &lt;- sapply(split(fcc_files_tidy[[\"file_release\"]], \n             fcc_files_tidy[[\"program_id\"]]), \n             unique) \n\ntype_proj &lt;- data.frame(\n  program_id = names(type_proj_temp),\n  type_proj  = type_proj_temp, \n  # a bit lazy and should be a join \n  file_release = file_release\n)\n\nfcc_all_dat &lt;- merge(fcc_all_dat, type_proj,\n      by.x = \"program_id\", by.y = \"program_id\", \n      all.x = TRUE, all.y = TRUE)\n\ntable_with_options(fcc_all_dat)",
    "crumbs": [
      "Home",
      "FCC Data",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "fcc_funding.html#from-fcc-program-csv",
    "href": "fcc_funding.html#from-fcc-program-csv",
    "title": "FCC Funding Map",
    "section": "From FCC program csv",
    "text": "From FCC program csv\nI just stacked them:\ncsvstack data/programdata_program* &gt; data/all_program.csv\nThen remove their elig_rules_desc and program_desc so it can fit in a table.\n\n\nCode\nall_prog &lt;- read.csv(\"data/all_program.csv\")\nlist_of_names_to_keep &lt;- c(\"agency_name\" ,  \"program_id\",\n                          \"program_start_date\",\"program_end_date\",    \"funding_source\",   \"funding_type\",        \"funding_obligated\",  \"funding_disbursed\",   \"funding_defaulted\",   \"min_download_spd\",    \"min_upload_spd\",     \"low_latency\",      \"funding_grant\",       \"program_cost\",       \"funding_loan\",        \"assistance_listings\", \"program_acronym\",     \"program_url\"  \n)\ntable_with_options(all_prog[, list_of_names_to_keep])",
    "crumbs": [
      "Home",
      "FCC Data",
      "FCC Funding Map"
    ]
  },
  {
    "objectID": "ntia_test.html",
    "href": "ntia_test.html",
    "title": "Testing other eligibilities",
    "section": "",
    "text": "Code\nsource(\"R/table_with_options.R\")\nsource(\"https://gist.githubusercontent.com/defuneste/32d5bffce8c6e88b6322ca4c9861793b/raw/893462fed28362601e63434ecc140bbdcbf6928b/us_states_df.R\")\nThe goal of this page is to record the effect on not including some technologies on the eligibility of BSL and census block.",
    "crumbs": [
      "Home",
      "FCC Data",
      "Testing other eligibilities"
    ]
  },
  {
    "objectID": "ntia_test.html#excluding-dsl-service",
    "href": "ntia_test.html#excluding-dsl-service",
    "title": "Testing other eligibilities",
    "section": "Excluding DSL service",
    "text": "Excluding DSL service\n\n\nCode\nselect\n    geoid_st,\n    sum(cnt_total_locations) as cnt_total_locations,\n    sum(cnt_underserved) as cnt_underserved,\n    sum(cnt_underserved_dsl_excluded) as cnt_loc_underserved_dsl_excluded,\n    count(*) as cnt_blocks,\n    sum(case when bl_100_20_area = 'underserved_area' then 1 else 0 end) as cnt_underserved_block,\n    sum(case when bl_100_20_area_dsl_excluded = 'underserved_area' then 1 else 0 end) as cnt_underserved_block_dsl_excluded\nfrom staging.bead_source_tiger_2020_blocks\ngroup by geoid_st\norder by geoid_st\n\n\nThis table will summarize some effects on locations and census blocks categories.\nOur previous definition:\n\nWe are excluding satellites and unlicensed wireless\nIf a location has only services lower than 25/3 it is unserved. If a location only has services with speed between 25/3 and 100/20 it is underserved. It will be served if it equal and above 100/20.\n\nNew definition:\n\nWe are now excluding DSL (technology 10).\nUnserved are not changing (if they had DSL under 25/3 it was already unserved). Underseved and Served will change because some locations who were served will now move to underserved.\n\nThis was for locations. If we move to census block (where we apply this “uncommun” 80/20) it will also change the category of eligibility (because we are changing the classification of locations).\nTable dicvtionnary:\n\n“United.States.of.America”: names of States\n“geoid_st: ANSI”number”\n“number_block”: Number of Census Block per states\n“cnt_underserved_block” : count of blocks underserved with the previous definition\n“cnt_underserved_block_dsl_excluded”: count of block if we exclude DSL\n“diff_block” : cnt_underserved_block_dsl_excluded - cnt_underserved_block\n“cnt_total_locations”: count of total locations per States\n“cnt_underserved”: count of locations underserved with previous definition\n“cnt_loc_underserved_dsl_excluded”: count of locations underserved if we removed DSL\n\nI provided the locations to:\n\ndo a bit of sanity check\nWorking at the block level imply using the 80/20 rules and kind of assum all block are the same.\n\n\n\nCode\neligibiliy_st_ntia &lt;- read.csv(\"data/dsl-exluded.csv\", colClasses =c(\"geoid_st\" = \"character\"))\nUS_slim &lt;- US_states[,c(\"United.States.of.America\", \"ANSI_num\", \"ANSI_let\")]\neasy_table &lt;- merge(eligibiliy_st_ntia, US_slim, \n                    by.x = \"geoid_st\", by.y = \"ANSI_num\",\n                    all.x = TRUE, all.y = TRUE)\n\neasy_table$diff_block &lt;- easy_table$cnt_underserved_block_dsl_excluded - easy_table$cnt_underserved_block\n\neasy_table &lt;- easy_table[, c(\"United.States.of.America\",\n                            \"geoid_st\",\n                            \"number_block\", \n                            \"cnt_underserved_block\",\n                            \"cnt_underserved_block_dsl_excluded\",\n                            \"diff_block\",\n                            \"cnt_total_locations\",\n                            \"cnt_underserved\",\n                            \"cnt_loc_underserved_dsl_excluded\"\n                            )]\n\ntable_with_options(easy_table)",
    "crumbs": [
      "Home",
      "FCC Data",
      "Testing other eligibilities"
    ]
  },
  {
    "objectID": "ntia_test.html#some-tldr",
    "href": "ntia_test.html#some-tldr",
    "title": "Testing other eligibilities",
    "section": "Some tl:dr:",
    "text": "Some tl:dr:\nNot keeping DSL move 26907 blocks from being served to be underserved.\nIt has heterogneous effects: for some States it has nearly or very low impacts but for other it is importants.",
    "crumbs": [
      "Home",
      "FCC Data",
      "Testing other eligibilities"
    ]
  },
  {
    "objectID": "rdof.html",
    "href": "rdof.html",
    "title": "RDOF EDA",
    "section": "",
    "text": "We had some previous works on RDOF for BCAT. It used shapefiles and gpkg to produce the output. This was not needed anymore as we could relly on census boundary (with less topological error) and just use the spreadsheet profided (.xls).\nIf we were about to change our workflow it was a good opportunity to test the end results.\nThe R script used for that can be find in the doc section of data-bead-etl repo.\nHere I will summarize some of the key results and dive a bit more on the trouble we ran into."
  },
  {
    "objectID": "rdof.html#key-results",
    "href": "rdof.html#key-results",
    "title": "RDOF EDA",
    "section": "Key results:",
    "text": "Key results:\n\nWe do not have any difference between our previous pipeline and the new one for “Authorized RDOF”\nIn 2024 they are no difference in “ready to authorized” to “authorized”: everything was authorized.\nWe also verified that RDOF data is matching to a Census block 2010 geoid.\nWe have data issue between “default”: ie we have two contradictory data sources."
  },
  {
    "objectID": "rdof.html#two-data-sources",
    "href": "rdof.html#two-data-sources",
    "title": "RDOF EDA",
    "section": "Two data sources:",
    "text": "Two data sources:\nOur previous works was on a release from the end of 2022 (16dec2022 according to the file name) and the new one end of 2023 (2023-12-20 according to the file name).\nThe version of 2022 have 266 994 and 286 892 for 2023. Our first thought was that it makes sense to have nore “default” as time go but some cases of default in 2022 are missing in 2023 (is it possible for a default RDOF to be not defaulted?).\n48 121 rows are only present in 2022 (they are the potentially weirds one). 68 019 rows are only present in 2023 version. Finally, 218873 are identics.\n\nOther small “hiccups”\nWe have some cases that were defaulted but still are present in authorized.: 47 cases, all in Wisconsin (County: 55043) and same company.\nThey will be removed from Authorized.\n\n\nUsing High Cost: Connect America Fund Broadband Map (CAF Map)\nThis data has a column for “Fund type” and one of this type is RDOF. It has 447 939 rows matching for RDOF. Unlike RDOF this data set is at the “location” scale, ie we can have multiple location per block.\nWe only get 54 825 Census Blocks, RDOF authorized has 489 811.\nThe vast majority of those are already present in RDOF data set, only 175 are ony present in the CAF data set.\nAccording to the metadata it seems that the CAF data set was produced in september 2023 but they did not provide their sources. It is hard to tract what was used.\nOur recomendation: use FCC source has much has possible and be mindfull when using other data sets."
  },
  {
    "objectID": "zero_dl_up.html",
    "href": "zero_dl_up.html",
    "title": "EDA on 0/0 BSL",
    "section": "",
    "text": "TODO: update the SQL query\nCode\nsource(\"R/table_with_options.R\")\n# very lazish function, col should be a string \nagg_count &lt;- function(dat, col) {\n    agg &lt;- aggregate(cbind(count = dat$count),\n                     list(name_col = dat[[col]]),\n                      sum)\n    colnames(agg) &lt;- c(col, \"count\")\n    return(agg)\n}\nThe goals of this page is storing a quick EDA about broadband services locations with 0 MBps uploads and 0 MBps downloads. To be concise we are going to call them 0/0 speeds.\nWe have counted every services that have been declared with 0/0 speeds and associated with their States, ISP and technology. To clarify that does not mean a location have 0/0 speeds only but that one “ISP x technology” is provided with this kind of service in this location.\nThe data used to provide most of the analysis was done with this 2 SQL queries. They were saved and stored in data/\nCode\nSELECT \n    state_abbr,\n    brand_name,\n    technology,\n    count(brand_name)\nFROM\n    staging.june23\nWHERE\n(max_advertised_download_speed = 0 AND\n    max_advertised_upload_speed = 0) = true\nGROUP BY brand_name, state_abbr, technology;\n\n-- first get all 0/0 then get all the non 0/0\n\nSELECT \n    state_abbr,\n    brand_name,\n    technology,\n    count(brand_name)\nFROM \n    staging.june23\nWHERE\n(max_advertised_download_speed = 0 AND\n    max_advertised_upload_speed = 0) = false\nGROUP BY brand_name, state_abbr, technology;\nCode\nzero_loc &lt;- read.csv(\"data/zero_dl_up.csv\")\nnot_zero &lt;- read.csv(\"data/not_zero_dl.csv\")",
    "crumbs": [
      "Home",
      "FCC Data",
      "EDA on 0/0 BSL"
    ]
  },
  {
    "objectID": "zero_dl_up.html#summary-by-technologies",
    "href": "zero_dl_up.html#summary-by-technologies",
    "title": "EDA on 0/0 BSL",
    "section": "Summary by technologies:",
    "text": "Summary by technologies:\n\n\nCode\nagg &lt;- agg_count(zero_loc, \"technology\") \nagg_not &lt;- agg_count(not_zero, \"technology\")\n\ntechnology &lt;- merge(agg, agg_not, by.x = \"technology\", \n                    by.y = \"technology\", all.x = TRUE, all.y = TRUE) \ncolnames(technology) &lt;- c(\"technology\",  \"cnt_zero_dl\", \"cnt_non_zero\")\ntechnology$rate_zero &lt;- round(technology$cnt_zero_dl / \n                (technology$cnt_zero_dl +  technology$cnt_non_zero), 4)\n\ntable_with_options(technology)\n\n\n\n\n\n\n\nWe do not mind too much 70 (Unlicensed Terrestrial Fixed Wireless) because we are filtering it out but we are keeping 71 (Licensed Terrestrial Fixed Wireless) , 72 (Licensed-by-Rule Terrestrial Fixed Wireless)and 10 (Copper Wire).\nTo take that into account I will filter out Unlicensed Terrestrial Fixed Wireless for the rest of this document. I also filtered out 60 and 61 to be consistant with our pipelines.",
    "crumbs": [
      "Home",
      "FCC Data",
      "EDA on 0/0 BSL"
    ]
  },
  {
    "objectID": "zero_dl_up.html#summary-by-isp",
    "href": "zero_dl_up.html#summary-by-isp",
    "title": "EDA on 0/0 BSL",
    "section": "Summary by ISP",
    "text": "Summary by ISP\n\n\nCode\nfilter_sat &lt;- c(60, 61, 70)\nzero_loc &lt;- zero_loc[which(! zero_loc$technology %in% filter_sat), ]\nnot_zero &lt;- not_zero[which(! not_zero$technology %in% filter_sat), ]\n\nagg &lt;- agg_count(zero_loc, \"brand_name\")\nagg_not &lt;- agg_count(not_zero, \"brand_name\")\n\nrate_zero &lt;- merge(agg, agg_not, \n                   by.x = \"brand_name\", by.y = \"brand_name\"\n                   , all.x = TRUE) \n\ncolnames(rate_zero) &lt;- c(\"brand_name\", \"cnt_zero_dl\", \"cnt_non_zero\")\nrate_zero$rate_zero &lt;- round(rate_zero$cnt_zero_dl /\n                 (rate_zero$cnt_zero_dl +  rate_zero$cnt_non_zero),\n                            4)\n\ntable_with_options(rate_zero[\n                    order(rate_zero$cnt_zero_dl, decreasing = TRUE),])\n\n\n\n\n\n\n\n\n402 ISPs are declaring services with 0/0 MBips (We have 2902 ISPs registered in FCC NBM)",
    "crumbs": [
      "Home",
      "FCC Data",
      "EDA on 0/0 BSL"
    ]
  },
  {
    "objectID": "zero_dl_up.html#sumamry-by-states",
    "href": "zero_dl_up.html#sumamry-by-states",
    "title": "EDA on 0/0 BSL",
    "section": "Sumamry by States",
    "text": "Sumamry by States\n\n\nCode\nst_agg_zero &lt;- agg_count(zero_loc, \"state_abbr\")\nst_agg_not &lt;- agg_count(not_zero, \"state_abbr\")\n\nst_agg &lt;- merge(st_agg_zero, st_agg_not, \n                by.x = \"state_abbr\", by.y = \"state_abbr\",\n                all.x = TRUE, all.y = TRUE)\ncolnames(st_agg) &lt;- c(\"ST\", \"cnt_zero_dl\", \"cnt_non_zero\")\nst_agg$rate_zero &lt;- round(st_agg$cnt_zero_dl /\n                    (st_agg$cnt_zero_dl +  st_agg$cnt_non_zero),\n                            4) \nlibrary(ggplot2)\n\nggplot(st_agg[!is.na(st_agg$rate_zero),], aes(rate_zero)) +\n  geom_boxplot(orientation = \"y\",\n  fill='#A4A4A4', color=\"black\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ntable_with_options(st_agg[order(st_agg$rate_zero, decreasing = TRUE), ])\n\n\n\n\n\n\n One point of concern is that services with 0/0 speeds could be generated for various reasons. One could be that some technology offer very low downloads/uploads and that is rounding to 0 an other could be that the location is not actually deserved but the ISP think it can do it.",
    "crumbs": [
      "Home",
      "FCC Data",
      "EDA on 0/0 BSL"
    ]
  },
  {
    "objectID": "road-eda.html",
    "href": "road-eda.html",
    "title": "TIGER Roads",
    "section": "",
    "text": "Requirement:",
    "crumbs": [
      "Home",
      "Other data sources",
      "TIGER Roads"
    ]
  },
  {
    "objectID": "road-eda.html#what-is-inside-the-data",
    "href": "road-eda.html#what-is-inside-the-data",
    "title": "TIGER Roads",
    "section": "what is inside the data:",
    "text": "what is inside the data:\nWe have linestrings with those attributes:\n\nlinearid: primary id, used by ither TIGER product\nfullname: road name, humane-readable\nrttyp: route type code, type of road.\n\n\n\n\nRoute Type Code\nRoute Type Code Description\n\n\n\n\nC\nCounty\n\n\nI\nInterstate\n\n\nM\nCommon Name\n\n\nO\nOther\n\n\nS\nState recognized\n\n\nU\nU.S.\n\n\n\n\nmtfcc: MAF/TIGER Feature Class Codes, example, S1400 (Local Neighborhood Road, Rural Road, City Street)\n\n\nClasses of Roads according to TIGER/line census\n\n\nTIGER: Topologically Integrated Geographic Encoding and Referencing\n\n\n\n\n\n\n\nmtfcc\nFeatures\n\n\n\n\n“S1100”\nPrimary Road\n\n\n“S1200”\nSecondary Road\n\n\n“S1400”\nLocal Neighborhood Road, Rural Road, City Street\n\n\n“S1500”\nVehicular Trail (4WD)\n\n\n“S1630”\nRamp\n\n\n“S1640”\nService Drive usually along a limited access highway\n\n\n“S1710”\nWalkway/Pedestrian Trail\n\n\n“S1720”\nStairway\n\n\n“S1730”\nAlley\n\n\n“S1740”\nPrivate Road for service vehicles (logging, oil fields, ranches, etc.)\n\n\n“S1750”\nInternal U.S. Census Bureau use\n\n\n“S1780”\nParking Lot Road\n\n\n“S1810”\nWinter Trail\n\n\n“S1820”\nBike Path or Trail\n\n\n“S1830”\nBridle Path",
    "crumbs": [
      "Home",
      "Other data sources",
      "TIGER Roads"
    ]
  },
  {
    "objectID": "road-eda.html#should-this-information-provided-and-how",
    "href": "road-eda.html#should-this-information-provided-and-how",
    "title": "TIGER Roads",
    "section": "Should this information provided and how?",
    "text": "Should this information provided and how?\nWe should probably not keep the bridle paths?\nShould it be provided at block level: ie an estimate route miles?\nHow the user will interact with it?\n\nResources\n\nhttps://www2.census.gov/geo/pdfs/reference/mtfccs2020.pdf\nhttps://www.census.gov/library/reference/code-lists/route-type-codes.html",
    "crumbs": [
      "Home",
      "Other data sources",
      "TIGER Roads"
    ]
  },
  {
    "objectID": "ms-eda.html",
    "href": "ms-eda.html",
    "title": "Microsoft Building footprint",
    "section": "",
    "text": "Why are we doing it?\nCurrently we have FCC total count of locations (BSL) at the census block level.",
    "crumbs": [
      "Home",
      "Other data sources",
      "Microsoft Building footprint"
    ]
  },
  {
    "objectID": "ms-eda.html#overview",
    "href": "ms-eda.html#overview",
    "title": "Microsoft Building footprint",
    "section": "Overview",
    "text": "Overview\nMicrosoft (MS) used satellite data (from multiple campains/vintage) to get the footprint of buildings.\nThey are classifying pixels that are supposed to be part of a buildings (segmentation using a neural network) and then convert pixels to a shape.\nIt exists worldwide and for US states.\n\nWe have processed the 51 states\nAdditional works will be required for Puero Rico and the US territories (parts of workflow from the US states can be reused).\n\nThe precision of their model vary depending on the region: the Carribean is at 92,2% and the US at 98.5%. The rate of false positive is 1% for the US and 1.8% for the Carribean. (Oceania was not provided)\nThe license is ODbL.",
    "crumbs": [
      "Home",
      "Other data sources",
      "Microsoft Building footprint"
    ]
  },
  {
    "objectID": "ms-eda.html#buildings-to-bsl",
    "href": "ms-eda.html#buildings-to-bsl",
    "title": "Microsoft Building footprint",
    "section": "Buildings to BSL?",
    "text": "Buildings to BSL?\nBuildings are shapes, BSL are points. We converted the buildings to single point (arbitrary: the first vertex of the shape) to lower the amout of data. Hence, now we have “buildings” summarized to points (lat/long).\n\n\nOur pipeline can be find here (private repo)\nWe do not have access to lat/long of BSL (this is only part of the fabric data). Our assumptions is if a count at block match they are describing the same reality (we can’t do the “on the ground verification”).\nThe number of buildings reported for 51 states is: 130 099 920\nWhile is the number of BSL is: 114 074 438\n\nOther potential sources:\n\nOpenStreetMap\nFCC staff estimates: at census block level (only for US 51)\nACS households",
    "crumbs": [
      "Home",
      "Other data sources",
      "Microsoft Building footprint"
    ]
  },
  {
    "objectID": "ms-eda.html#ms-building-footprint-in-vt",
    "href": "ms-eda.html#ms-building-footprint-in-vt",
    "title": "Microsoft Building footprint",
    "section": "MS building footprint in VT",
    "text": "MS building footprint in VT\nWe can count those points per block and compare to the number of location than FCC is describing.\nAfter that we build a small model that will provide either an estimate of BSL given MS footprint and how confidant the model is.\n\ntot_loc ~ cnt_mscnt_ms ~ tot_loc\n\n\n\n\nCode\nvt &lt;- read.csv(\"data/vt_ms.csv\")\nvt$tot_loc &lt;-as.numeric(vt$tot_loc) \nvt$tot_loc &lt;- ifelse(is.na(vt$tot_loc), 0, vt$tot_loc)\n\nlm_loc &lt;- lm(tot_loc ~  cnt_ms , data = vt)\npot_val2 &lt;- seq(min(vt$cnt_ms), max(vt$cnt_ms), by = 0.025)\nconf_interval &lt;- predict(lm_loc, newdata = data.frame( cnt_ms = pot_val2) ,\n                         interval = \"prediction\", level = 0.95)\n\nplot(vt$cnt_ms, vt$tot_loc,  col = \"#DF536B50\", asp = 1, \nxlab = \"cnt of MS buildings\", ylab = \"cnt of fcc locations\") \nabline(lm_loc, col = 4)\nabline(1:500, 1:500, col = \"black\")\nlines(pot_val2, conf_interval[, \"lwr\"], col = \"blue\", lty = 2)\nlines(pot_val2, conf_interval[, \"upr\"], col =\" blue\", lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlm_ms &lt;- lm(cnt_ms ~ tot_loc, data = vt)\npot_val &lt;- seq(min(vt$tot_loc), max(vt$tot_loc), by = 0.025)\nconf_interval &lt;- predict(lm_ms, newdata = data.frame( tot_loc = pot_val) ,\n                         interval = \"prediction\", level = 0.95)\n\nplot(vt$tot_loc, vt$cnt_ms,  col = \"#DF536B50\", asp = 1, \nylab = \"cnt of MS buildings\", xlab = \"cnt of fcc locations\") \nabline(lm_ms, col = 4)\nabline(1:500, 1:500, col = \"black\")\nlines(pot_val, conf_interval[, \"lwr\"], col = \"blue\", lty = 2)\nlines(pot_val, conf_interval[, \"upr\"], col =\" blue\", lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrong relation\nthe model is “overconfidant”, and the reality is more “spread” (what could possibly be the cause of it?)\nVT MS has also more locations (285333, versus 352618)",
    "crumbs": [
      "Home",
      "Other data sources",
      "Microsoft Building footprint"
    ]
  }
]